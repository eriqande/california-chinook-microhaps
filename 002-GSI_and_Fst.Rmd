---
title: "Assessing Power of the California Microhap Baseline for GSI (and calculating Fst)"
author: "Eric C. Anderson"
date: "Last Updated: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_knit$set(root.dir = here::here())
start_time <- Sys.time()

USING_SNAKEMAKE <- FALSE
NEG_USING_SMK <- TRUE
if(exists("snakemake")) {
  USING_SNAKEMAKE <- TRUE
  NEG_USING_SMK <- FALSE
}
```

```{r rmd_snakemake_hacks, include=FALSE, eval=USING_SNAKEMAKE}
# currently, this block does not get evaluated, while developing,
# but later when we snakemake-ize it all, we will evaluate it.
if(USING_SNAKEMAKE) {
  input_list <- snakemake@input
  output_list <- snakemake@output
}
```

# Input and Output Paths

```{r input_bypass, include=NEG_USING_SMK, eval=NEG_USING_SMK}
# inputs:
input_list <- list(
  #final_baseline = "data/cali-chinook-baseline.rds",
  final_baseline = "data/subRoSA_baseline_rubias_FEB2024_w_mixture_revised.csv",
  pop_labels = "inputs/reference-collection-names.csv",
  map_notations = "inputs/map-notations.tsv"
)
# outputs:
output_list <- list(
  assig_table = "results/GSI_and_Fst/assignment-table-full-baseline.pdf",
  fst_table = "results/GSI_and_Fst/full-baseline-but-no-lfar-Fst-table.pdf",
  assig_table_lfar_all = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar.pdf",
  assig_table_lfar_no_miss = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar-and-everyone-has-the-lfar.pdf",
  tex_table = "tex/inputs/samples-table.tex",
  gsi_fst_fig = "results/GSI_and_Fst/gsi_and_fst_fig.pdf",
  gsi_fst_fig_crop = "results/GSI_and_Fst/gsi_and_fst_fig-crop.pdf",
  gsi_fst_fig_tex = "tex/images/gsi_and_fst_fig-crop.pdf"
)
# you can create the necessary output directories like this:
dump <- lapply(output_list, function(x)
  dir.create(dirname(x), recursive = TRUE, showWarnings = FALSE)
)
```


# Introduction

Here we assess power of the baseline for population assignment

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rubias)
library(cowplot)
library(hierfstat)
library(lubridate)


# get the full baseline with the new markers, etc.  We will toss out some
# non-winter Sacto fish of unknown provenance
full_base <- read_csv(input_list$final_baseline) %>%
  filter(collection != "SacNonW")


# read the pop labels for proper sorting
pop_labels <- read_csv(input_list$pop_labels)

# rename those collections and repunits
full_base2 <- full_base %>%
  rename(old_name = collection) %>%
  select(-repunit) %>%
  left_join(pop_labels %>% select(old_name, collection, repunit), by = join_by(old_name)) %>%
  select(-old_name) %>%
  select(indiv, repunit, collection, sample_type, everything()) %>%
  filter(!duplicated(indiv))  # necessary cuz clemento stuck mill and deer together into a single code
  
# finally, get the order of collections and repunits we want
collection_order <- unique(pop_labels$collection)
```


# Make a table summary of all the samples

I would like to break things down by general sampling location, run type, number of samples,
month-range of sampling, and range of years.
```{r}
meta <- read_csv("data/baseline_repository_meta_complete.csv.gz") %>%
  rename(indiv = NMFS_DNA_ID...1) %>%
  select(
    indiv, REPORTED_LIFE_STAGE, PHENOTYPE, COLLECTION_DATE, ESTIMATED_DATE, WATERSHED, TRIB_1,
    TRIB_2, WATER_NAME, REACH_SITE, HATCHERY, LATITUDE_F,
    LONGITUDE_F, LOCATION_COMMENTS_F
  ) %>%
  mutate(
    collection_date = mdy(COLLECTION_DATE),
    month = month(collection_date),
    year = year(collection_date)
  )


# get the notations on the map, and, in particular get the distinct values of
# abbrv and name_text
map_notations <- read_tsv(input_list$map_notations)

map_notations_dist1 <- map_notations %>%
  distinct(abbrv, repository_name, map_text, run_timing, run_timing_for_table)


# join the baseline to the meta data and standardize names and nest everything on abbrv and repository name
fbmet <- full_base2 %>%
  select(1:4) %>%
  left_join(meta, by = join_by(indiv)) %>%
  rename(abbrv = collection) %>%
  mutate(
    repository_name = case_when(
      !is.na(HATCHERY) ~ HATCHERY,
      TRUE ~ WATER_NAME
    )
  ) %>%
  left_join(map_notations_dist1, by = join_by(abbrv, repository_name)) %>%
  group_by(abbrv, repository_name, run_timing_for_table, map_text, WATERSHED) %>%
  nest()

# Remove this once I get the full meta data
fbmet <- fbmet %>% filter(!is.na(WATERSHED))


# then extract bits of information from the data and set sparkline paths
fbmet2 <- fbmet %>%
  mutate(
    N = map_int(.x = data, .f = function(x) nrow(x)),
    N_date = map_int(.x = data, .f = function(x) sum(!(is.na(x$collection_date)) & yday(x$collection_date) != 1) ),  # we use YYYY-01-01 to denote year is known but day is missing in the data repository, so here we filter those out.
    `Run Timing` = map_chr(.x = data, .f = function(x) x$run_timing[1]),
    min_year = map_int(.x = data, .f = function(x) base::min(x$year, na.rm = TRUE)),
    max_year = map_int(.x = data, .f = function(x) base::max(x$year, na.rm = TRUE)),
    sparkline_path = str_c("tex/images/months-", abbrv, "_", str_replace_all(repository_name, " ", "-"), ".pdf")
  )


# then make the images.  Put them right into the tex directory
source("R/month_sample_sparkline.R")

dir.create("tex/images", recursive = TRUE, showWarnings = FALSE)

fbmet3 <- fbmet2 %>%
  mutate(
    ggplot = map2(.x = data, .y = sparkline_path, .f = function(x, y) month_sample_sparkline(tib = x, path = y))
  )

# now, add all the columns we might need.
fbmet4 <- fbmet3 %>%
  left_join(map_notations %>% select(abbrv, repository_name, map_text, basin, ESU, `Mainstem River` ))

# and now do some formatting
fbmet5 <- fbmet4 %>%
  ungroup() %>%
  mutate(
    `Sampling Months` = sprintf("\\raisebox{-0.12 em}{\\includegraphics[height=1.02em]{../%s}}", sparkline_path),
    `Year Range` = sprintf("%d--%d", min_year, max_year)
  ) %>%
  select(`Mainstem River`, abbrv, map_text, run_timing_for_table, ESU, N, N_date, `Sampling Months`, `Year Range`) %>%
  mutate(map_text = str_replace(map_text, ",.*$", "")) %>% # this removes the abbreviations after the commas on the map names
  mutate(N_date = replace_na(N_date, 0L)) %>%
  rename(
    `Mainstem River$^1$` = `Mainstem River`,
    Code = abbrv,
    `Location Name$^2$` = map_text,
    Run = run_timing_for_table,
    `ESU$^3$` = ESU,
    `$N$` = N,
    `$N_\\mathrm{day}^4$` = N_date
  ) %>%
  arrange(factor(Code, levels = collection_order))

# a hack to get an hline under the heading:
fbmet5$`Mainstem River$^1$`[1] <- str_c("\\hline ", fbmet5$`Mainstem River$^1$`[1])

# Finally write out the table
write_delim(fbmet5, delim = "&", eol = "\\tabularnewline\n", file = output_list$tex_table)



```




# Do the self-assignment

In this round, we go directly to assigning to repunit, and then to the max-likelihood
collection within that repunit.

```{r}
full_sa <- self_assign(full_base2, gen_start_col = 5)
```

```{r}
top_ass <- full_sa %>%
  group_by(indiv, collection, repunit, inferred_repunit) %>%
  mutate(repu_sclike = sum(scaled_likelihood)) %>%
  group_by(indiv) %>%
  filter(repu_sclike == max(repu_sclike)) %>%
  arrange(indiv, desc(scaled_likelihood)) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(coll_f = factor(collection, levels = collection_order))
```

And, once again, we want to investigate the ones that are in the wrong repunit, and look at their
scaled_repu_likelihoods, the others.
```{r}
source("R/colors.R")
ta_corr <- top_ass %>%
  filter(repunit == inferred_repunit)
ta_wrong <- top_ass %>%
  filter(repunit != inferred_repunit) %>%
  arrange(collection, repu_sclike) %>%
  group_by(collection) %>%
  mutate(y = 190 + (70/9) * 1:n())

ggplot() +
  geom_histogram(
    data = ta_corr, 
    mapping = aes(x = repu_sclike, fill = inferred_repunit)
  ) +
  geom_point(
    data = ta_wrong,
    mapping = aes(x = repu_sclike, y = y, fill = inferred_repunit),
    size = 3, 
    shape = 21
  ) +
  facet_wrap(~coll_f, ncol = 3) +
  scale_fill_manual(values = repunit_colors)
```
Which looks to me like there are really only 7 here that we are not getting right.
Those others we will filter out and make a table of them:

```{r}
tafilt <- top_ass %>%
  filter(!(repunit != inferred_repunit & repu_sclike > 0.99))
```

Here we calculate the number of correctly and incorrectly assigned fish:
```{r}
tafilt %>%
  mutate(correct = repunit == inferred_repunit) %>%
  count(correct) %>%
  mutate(fract = n / sum(n))
```

And now we will make a table of those:
```{r}
# get the function
source("R/taf-prep.R")
source("R/table-as-figure.R")

# get the RC_groups
RC_groups <- pop_labels %>%
  select(-old_name) %>%
  distinct() %>%
  rename(external_group = run_timing_group, internal_group = repunit, label = collection) %>%
  select(external_group, internal_group, label)

X2 <- tafilt %>%
  mutate(
    collection_f = factor(collection, levels = collection_order),
    inferred_coll_f = factor(inferred_collection, levels = collection_order)
  ) %>%
  count(collection_f, inferred_coll_f, .drop = FALSE) %>%
  taf_prep()

Rubias_for_later <- X2 # keeping this for later...

# get the result
TAF2 <- table_as_figure(
  X = X2, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors, 
  plot_margins = c(1, 0.1, 0.7, 0.1)
)

# and plot it
g <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  nrow = 2,
  rel_heights = c(7,3)
)

g
```

That is a considerable improvement. Let's write it out:
```{r}
ggsave(g, filename = output_list$assig_table, width = 8, height = 10)
```


# Calculating Fst between the collections from the microhaplotype markers

We use the 'hierfstat' package to get the pairwise
$F_\mathrm{ST}$ values (Weir & Cockerham method) between the collections from these microhaplotype data.

In order to do this we need to convert the data to a lumped format.
This means making integers of the alleles and then catenating them.
One locus has 11 allele, so we should use two digits for each allele.  We drop the
LFAR from this, because those were chosen specifically to be weird in the late fall.
```{r}
full_base_long <- full_base2 %>% 
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  mutate(
    Pop = as.integer(factor(collection, levels = collection_order)),
    .before = collection
  ) %>%
  select(-repunit, -collection, -sample_type) %>%
  pivot_longer(
    cols = c(-indiv, -Pop),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  )

hfs_ref_long <- full_base_long %>%
  group_by(locus) %>%
  mutate(
    alle_int =  sprintf("%02d", as.integer(factor(value)))
  ) %>%
  ungroup() %>%
  select(-value)

# then we can lump the alleles 
hfs_ref_wide <- hfs_ref_long %>%
  group_by(indiv, Pop, locus) %>%
  summarise(geno = str_c(alle_int[1], alle_int[2])) %>%
  ungroup() %>%
  mutate(geno = ifelse(str_detect(geno, "NA"), NA_character_, geno)) %>%
  mutate(geno = as.integer(geno)) %>%
  pivot_wider(
    names_from = c(locus),
    values_from = geno
  ) %>%
  arrange(Pop, indiv)

hfstat_dat <- hfs_ref_wide %>% 
  select(-indiv) %>%
  as.data.frame()


FST <- pp.fst(dat = hfstat_dat, diploid = TRUE)

# let's also bootstrap that:
FST_boot <- boot.ppfst(
  dat = hfstat_dat,
  nboot = 1000,
  quant = c(0.025,0.975),
  diploid = TRUE
)


# now, get the values out of it and put them in the upper diagonal
# of a matrix for plotting with the WGS values.
fst_mat <- FST$fst.pp 
colnames(fst_mat) <- collection_order
```


```{r}
  
all_values_tib <- fst_mat %>%
  as_tibble() %>%
  mutate(row_label = collection_order, .before = SRW) %>%
  pivot_longer(
    -row_label, 
    names_to = "col_label",
    values_to = "fst"
  )

# to get it to the right format we make it look like rubias
# output and the pipe it into taf_prep()
upper_tri_fst <- all_values_tib %>%
  filter(!is.nan(fst)) %>%
  mutate(
    collection_f = factor(row_label, levels = collection_order),
    inferred_coll_f = factor(col_label, levels = collection_order)
  ) %>% 
  select(-row_label, -col_label) %>%
  mutate(n = sprintf("%0.3f", fst)) %>%
  select(-fst) %>%
  taf_prep()
  
 
```


Now that we have the `upper_tri_fst` values in the right format, we just need
to add values to the lower triangle part of the matrix.  


# Adding allele frequency scatter plots to the lower diagonal

I thought about doing differentiation tests, or histograms of per-locus
Fst values, but then I realized that it actually might be way more informative
to put small-multiple scatter plots of allele frequencies in each of the lower-diagonal
cells of the Fst figure.  I think it might look cool.

We will also standardize each sample size (at each locus) to the minimum
number of observed genotypes in a population.


## Calculate allele frequencies for each population and standardize to lowest sample size

Straightforward tidyverse stuff:
```{r}

# long format with no missing data
long_genos <- full_base2 %>%
  select(-repunit, -sample_type) %>%
  pivot_longer(
    cols = -c(indiv, collection),
    names_to = c("locus", "gene_copy"),
    values_to = "allele",
    names_pattern = "(.*)[_.]([12])$"
  ) %>%
  filter(!is.na(allele))

# now get the minimum sample sizes for each
min_sampled <- long_genos %>%
  count(collection, locus) %>%
  group_by(locus) %>%
  summarise(min_ngc = min(n))

# having looked at those, if we keep anything with
# at least 38 gene copies, we have most of the loci.
# So, we will use this to slice sample some individuals from
# each population. We have to do a bunch of rigamoral because
# slice_sample requires that the n argument is a constant.  BOGUS!
set.seed(5)
long_genos_subsampled <- long_genos %>%
  left_join(min_sampled, by = "locus") %>%
  filter(min_ngc >= 38) %>%
  group_by(collection, locus, min_ngc) %>%
  nest() %>%
  mutate(
    ssd = map2(.x = data, .y = min_ngc, .f = function(x, y) slice_sample(x, n = y))
  ) %>%
  select(-data) %>%
  unnest(ssd) %>%
  select(-min_ngc)

  

# get allele frequencies
alle_freqs <- long_genos_subsampled %>%
  count(collection, locus, allele) %>%
  group_by(collection, locus) %>%
  mutate(freq = n / sum(n)) %>%
  select(-n) %>%
  ungroup()


# get all pairwise comparisons of population allele frequencies
backbone <- expand_grid(
  row_coll =  unique(alle_freqs$collection),
  col_coll = unique(alle_freqs$collection),
  alle_freqs %>% distinct(locus, allele)
)

# now get the freqs on there, order the collections and ensure
# that we are on the diagonal and the lower triangle, and then also get
# the positions for each of the points.  We will allow that the whole
# scatterplot can be inset into the cell by a fractional amount inset
inset <- 0.15
pair_freqs <- backbone %>%
  left_join(alle_freqs, by = join_by(row_coll == collection, locus, allele)) %>%
  rename(row_freq = freq) %>%
  left_join(alle_freqs, by = join_by(col_coll == collection, locus, allele)) %>%
  rename(col_freq = freq) %>%
  mutate(
    row_coll = factor(row_coll, levels = collection_order),
    col_coll = factor(col_coll, levels = collection_order)
  ) %>%
  filter(as.integer(col_coll) <= as.integer(row_coll)) %>%
  replace_na(list(row_freq = 0.0, col_freq = 0.0)) %>%
  mutate( # this is just setting things as in table_as_figure
      ymin = nlevels(row_coll) - as.integer(row_coll) + inset,
      ymax = nlevels(row_coll) - as.integer(row_coll) + 1 - inset,
      xmin = as.integer(col_coll) - 1 + inset,
      xmax = as.integer(col_coll) - inset
    ) %>%
  mutate(  # here we get the x-y positions for the scatter points
    x = xmin + col_freq * (xmax - xmin),
    y = ymin + row_freq * (ymax - ymin)
  )

```

Making a figure.
```{r}

# we need to get colors and things for the fill.  We just take the
# rubias output that was run through taf_prep and we keep the lower
# diagonal and we set all the cell_labels to NA.
lower_tri_base <- Rubias_for_later %>%
  filter(as.integer(col_label) <= as.integer(row_label)) %>%
  mutate(cell_label = NA_character_)

# then we put those together and make a figure
full_fst <- bind_rows(
  upper_tri_fst,
  lower_tri_base
) %>%
  arrange(row_label, col_label)


# get the result
TAF_FST <- table_as_figure(
  X = full_fst, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors,
  Xs_on_diagonal = FALSE,
  plot_margins = c(0.7, 0.1, 0.1, 0.1)
)


# then plot it and add the scatter points on the lower diagonal
fst_with_scatters <- TAF_FST$full_plot +
  geom_point(
    data = pair_freqs,
    mapping = aes(x = x, y = y),
    size = 0.025,
    colour = "black"
  )


```

And we can save that too:
```{r}
ggsave(fst_with_scatters, file = output_list$fst_table, width = 6, height = 5.1)
```

# Put them together into a single figure for the paper

```{r}
gsi_fst_fig_for_paper <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  fst_with_scatters,
  nrow = 3,
  rel_heights = c(6,1.5,6)
)

# and then save that
ggsave(gsi_fst_fig_for_paper, file = output_list$gsi_fst_fig, width = 10, height = 19)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$gsi_fst_fig, collapse = " ")
system(CALL)

file.rename(from = output_list$gsi_fst_fig, to = output_list$gsi_fst_fig_tex)
```
