---
title: "Assessing Power of the California Microhap Baseline for GSI (and calculating Fst)"
author: "Eric C. Anderson"
date: "Last Updated: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_knit$set(root.dir = here::here())
start_time <- Sys.time()

USING_SNAKEMAKE <- FALSE
NEG_USING_SMK <- TRUE
if(exists("snakemake")) {
  USING_SNAKEMAKE <- TRUE
  NEG_USING_SMK <- FALSE
}
```

```{r rmd_snakemake_hacks, include=FALSE, eval=USING_SNAKEMAKE}
# currently, this block does not get evaluated, while developing,
# but later when we snakemake-ize it all, we will evaluate it.
if(USING_SNAKEMAKE) {
  input_list <- snakemake@input
  output_list <- snakemake@output
}
```

# Input and Output Paths

```{r input_bypass, include=NEG_USING_SMK, eval=NEG_USING_SMK}
# inputs:
input_list <- list(
  #final_baseline = "data/cali-chinook-baseline.rds",
  final_baseline = "data/subRoSA_baseline_rubias_FEB2024_w_mixture_revised.csv",
  lfar_genos = "data/metagenos_hap2col_lfar_all_samps.csv",
  pop_labels = "inputs/reference-collection-names.csv",
  map_notations = "inputs/map-notations.tsv"
)
# outputs:
output_list <- list(
  assig_table = "results/GSI_and_Fst/assignment-table-full-baseline.pdf",
  fst_table = "results/GSI_and_Fst/full-baseline-but-no-lfar-Fst-table.pdf",
  assig_table_lfar_all = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar.pdf",
  tex_table = "tex/inputs/samples-table.tex",
  gsi_fst_fig = "results/GSI_and_Fst/gsi_and_fst_fig.pdf",
  gsi_fst_fig_crop = "results/GSI_and_Fst/gsi_and_fst_fig-crop.pdf",
  gsi_fst_fig_tex = "tex/images/gsi_and_fst_fig-crop.pdf"
)
# you can create the necessary output directories like this:
dump <- lapply(output_list, function(x)
  dir.create(dirname(x), recursive = TRUE, showWarnings = FALSE)
)
```


# Introduction

Here we assess power of the baseline for population assignment

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rubias)
library(cowplot)
library(hierfstat)
library(lubridate)


# get the full baseline with the new markers, etc.  We will toss out some
# non-winter Sacto fish of unknown provenance
full_base <- read_csv(input_list$final_baseline) %>%
  filter(collection != "SacNonW")

lfar_genos <- read_csv(input_list$lfar_genos)

# read the pop labels for proper sorting
pop_labels <- read_csv(input_list$pop_labels)

# rename those collections and repunits
full_base2 <- full_base %>%
  rename(old_name = collection) %>%
  select(-repunit) %>%
  left_join(pop_labels %>% select(old_name, collection, repunit), by = join_by(old_name)) %>%
  select(-old_name) %>%
  select(indiv, repunit, collection, sample_type, everything()) %>%
  filter(!duplicated(indiv))  # necessary cuz clemento stuck mill and deer together into a single code
  
# finally, get the order of collections and repunits we want
collection_order <- unique(pop_labels$collection)
```


# Make a table summary of all the samples

I would like to break things down by general sampling location, run type, number of samples,
month-range of sampling, and range of years.
```{r}
meta <- read_csv("data/baseline_repository_meta_complete.csv.gz") %>%
  rename(indiv = NMFS_DNA_ID...1) %>%
  select(
    indiv, REPORTED_LIFE_STAGE, PHENOTYPE, COLLECTION_DATE, ESTIMATED_DATE, WATERSHED, TRIB_1,
    TRIB_2, WATER_NAME, REACH_SITE, HATCHERY, LATITUDE_F,
    LONGITUDE_F, LOCATION_COMMENTS_F
  ) %>%
  mutate(
    collection_date = mdy(COLLECTION_DATE),
    month = month(collection_date),
    year = year(collection_date)
  )


# get the notations on the map, and, in particular get the distinct values of
# abbrv and name_text
map_notations <- read_tsv(input_list$map_notations)

map_notations_dist1 <- map_notations %>%
  distinct(abbrv, repository_name, map_text, run_timing, run_timing_for_table)


# join the baseline to the meta data and standardize names and nest everything on abbrv and repository name
fbmet <- full_base2 %>%
  select(1:4) %>%
  left_join(meta, by = join_by(indiv)) %>%
  rename(abbrv = collection) %>%
  mutate(
    repository_name = case_when(
      !is.na(HATCHERY) ~ HATCHERY,
      TRUE ~ WATER_NAME
    )
  ) %>%
  left_join(map_notations_dist1, by = join_by(abbrv, repository_name)) %>%
  group_by(abbrv, repository_name, run_timing_for_table, map_text, WATERSHED) %>%
  nest()

# Remove this once I get the full meta data
fbmet <- fbmet %>% filter(!is.na(WATERSHED))


# then extract bits of information from the data and set sparkline paths
fbmet2 <- fbmet %>%
  mutate(
    N = map_int(.x = data, .f = function(x) nrow(x)),
    N_date = map_int(.x = data, .f = function(x) sum(!(is.na(x$collection_date)) & yday(x$collection_date) != 1) ),  # we use YYYY-01-01 to denote year is known but day is missing in the data repository, so here we filter those out.
    `Run Timing` = map_chr(.x = data, .f = function(x) x$run_timing[1]),
    min_year = map_int(.x = data, .f = function(x) base::min(x$year, na.rm = TRUE)),
    max_year = map_int(.x = data, .f = function(x) base::max(x$year, na.rm = TRUE)),
    sparkline_path = str_c("tex/images/months-", abbrv, "_", str_replace_all(repository_name, " ", "-"), ".pdf")
  )


# then make the images.  Put them right into the tex directory
source("R/month_sample_sparkline.R")

dir.create("tex/images", recursive = TRUE, showWarnings = FALSE)

fbmet3 <- fbmet2 %>%
  mutate(
    ggplot = map2(.x = data, .y = sparkline_path, .f = function(x, y) month_sample_sparkline(tib = x, path = y))
  )

# now, add all the columns we might need.
fbmet4 <- fbmet3 %>%
  left_join(map_notations %>% select(abbrv, repository_name, map_text, basin, ESU, `Mainstem River` ))

# and now do some formatting
fbmet5 <- fbmet4 %>%
  ungroup() %>%
  mutate(
    `Sampling Months` = sprintf("\\raisebox{-0.12 em}{\\includegraphics[height=1.02em]{../%s}}", sparkline_path),
    `Year Range` = sprintf("%d--%d", min_year, max_year)
  ) %>%
  select(`Mainstem River`, abbrv, map_text, run_timing_for_table, ESU, N, N_date, `Sampling Months`, `Year Range`) %>%
  mutate(map_text = str_replace(map_text, ",.*$", "")) %>% # this removes the abbreviations after the commas on the map names
  mutate(N_date = replace_na(N_date, 0L)) %>%
  rename(
    `Mainstem River$^1$` = `Mainstem River`,
    Code = abbrv,
    `Location Name$^2$` = map_text,
    Run = run_timing_for_table,
    `ESU$^3$` = ESU,
    `$N$` = N,
    `$N_\\mathrm{day}^4$` = N_date
  ) %>%
  arrange(factor(Code, levels = collection_order))

# a hack to get an hline under the heading:
fbmet5$`Mainstem River$^1$`[1] <- str_c("\\hline ", fbmet5$`Mainstem River$^1$`[1])

# Finally write out the table
write_delim(fbmet5, delim = "&", eol = "\\tabularnewline\n", file = output_list$tex_table)



```






# Do the self-assignment

In this round, we go directly to assigning to repunit, and then to the max-likelihood
collection within that repunit.

```{r}
full_sa <- self_assign(full_base2, gen_start_col = 5)
```

```{r}
top_ass <- full_sa %>%
  group_by(indiv, collection, repunit, inferred_repunit) %>%
  mutate(repu_sclike = sum(scaled_likelihood)) %>%
  group_by(indiv) %>%
  filter(repu_sclike == max(repu_sclike)) %>%
  arrange(indiv, desc(scaled_likelihood)) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(coll_f = factor(collection, levels = collection_order))
```

And, once again, we want to investigate the ones that are in the wrong repunit, and look at their
scaled_repu_likelihoods, the others.
```{r}
source("R/colors.R")
ta_corr <- top_ass %>%
  filter(repunit == inferred_repunit)
ta_wrong <- top_ass %>%
  filter(repunit != inferred_repunit) %>%
  arrange(collection, repu_sclike) %>%
  group_by(collection) %>%
  mutate(y = 190 + (70/9) * 1:n())

ggplot() +
  geom_histogram(
    data = ta_corr, 
    mapping = aes(x = repu_sclike, fill = inferred_repunit)
  ) +
  geom_point(
    data = ta_wrong,
    mapping = aes(x = repu_sclike, y = y, fill = inferred_repunit),
    size = 3, 
    shape = 21
  ) +
  facet_wrap(~coll_f, ncol = 3) +
  scale_fill_manual(values = repunit_colors)
```
Which looks to me like there are really only 7 here that we are not getting right.
Those others we will filter out and make a table of them:

```{r}
tafilt <- top_ass %>%
  filter(!(repunit != inferred_repunit & repu_sclike > 0.99))
```

Here we calculate the number of correctly and incorrectly assigned fish:
```{r}
tafilt %>%
  mutate(correct = repunit == inferred_repunit) %>%
  count(correct) %>%
  mutate(fract = n / sum(n))
```

And now we will make a table of those:
```{r}
# get the function
source("R/taf-prep.R")
source("R/table-as-figure.R")

# get the RC_groups
RC_groups <- pop_labels %>%
  select(-old_name) %>%
  distinct() %>%
  rename(external_group = run_timing_group, internal_group = repunit, label = collection) %>%
  select(external_group, internal_group, label)

X2 <- tafilt %>%
  mutate(
    collection_f = factor(collection, levels = collection_order),
    inferred_coll_f = factor(inferred_collection, levels = collection_order)
  ) %>%
  count(collection_f, inferred_coll_f, .drop = FALSE) %>%
  taf_prep()

Rubias_for_later <- X2 # keeping this for later...

# get the result
TAF2 <- table_as_figure(
  X = X2, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors, 
  plot_margins = c(1, 0.1, 0.7, 0.1)
)

# and plot it
g <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  nrow = 2,
  rel_heights = c(7,3)
)

g
```

That is a considerable improvement. Let's write it out:
```{r}
ggsave(g, filename = output_list$assig_table, width = 8, height = 10)
```


# Now, we repeat that while adding the LFAR markers in there

We do these as an additional step to get a sense for how much
improvement we get from adding the LFAR in there.

First we need to join the LFARs on there:
```{r}
# get just the 3 LFAR markers we are using:
lfar_columns <- paste(
  rep(c(
  "NC_037130.1:1062935-1063235",
  "NC_037130.1:828619-828919",
  "NC_037130.1:864908-865208"
  ), each = 2),
  rep(c(1,2), 3),
  sep = "_"
)


lfar3genos <- lfar_genos[c("Sample_ID", lfar_columns)]


base_with_lfar <- full_base2 %>%
  left_join(lfar3genos, by = join_by(indiv == Sample_ID))
```

Then, all the step from above:
```{r}
# Do self-assignment:
full_plus_lfar_sa <- self_assign(base_with_lfar, gen_start_col = 5)


top_ass_w_lfar <- full_plus_lfar_sa %>%
  group_by(indiv, collection, repunit, inferred_repunit) %>%
  mutate(repu_sclike = sum(scaled_likelihood)) %>%
  group_by(indiv) %>%
  filter(repu_sclike == max(repu_sclike)) %>%
  arrange(indiv, desc(scaled_likelihood)) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(coll_f = factor(collection, levels = collection_order))



```

investigate the ones that are in the wrong repunit, and look at their
scaled_repu_likelihoods, the others.
```{r}
ta_corr_lfar <- top_ass_w_lfar %>%
  filter(repunit == inferred_repunit)
ta_wrong_lfar <- top_ass_w_lfar %>%
  filter(repunit != inferred_repunit) %>%
  arrange(collection, repu_sclike) %>%
  group_by(collection) %>%
  mutate(y = 190 + (70/9) * 1:n())

ggplot() +
  geom_histogram(
    data = ta_corr_lfar,
    mapping = aes(x = repu_sclike, fill = inferred_repunit)
  ) +
  geom_point(
    data = ta_wrong_lfar,
    mapping = aes(x = repu_sclike, y = y, fill = inferred_repunit),
    size = 3,
    shape = 21
  ) +
  facet_wrap(~coll_f, ncol = 3) +
  scale_fill_manual(values = repunit_colors)
```

Filter out the ones that look like clear mis-labelling and then
count up the mis-assigned fish:
```{r}
tafilt_w_lfar <- top_ass_w_lfar %>%
  filter(!(repunit != inferred_repunit & repu_sclike > 0.99))
```

Here we calculate the number of correctly and incorrectly assigned fish:
```{r}
tafilt_w_lfar %>%
  mutate(correct = repunit == inferred_repunit) %>%
  count(correct) %>%
  mutate(fract = n / sum(n))
```

Having the LFAR in there improves things. Good.

Make a table of those:
```{r}

X2_lfar <- tafilt_w_lfar %>%
  mutate(
    collection_f = factor(collection, levels = collection_order),
    inferred_coll_f = factor(inferred_collection, levels = collection_order)
  ) %>%
  count(collection_f, inferred_coll_f, .drop = FALSE) %>%
  taf_prep()

Rubias_for_later_lfar <- X2_lfar # keeping this for later...

# get the result
TAF2_lfar <- table_as_figure(
  X = X2_lfar,
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors,
  plot_margins = c(1, 0.1, 0.7, 0.1)
)

# and plot it
g_lfar <- plot_grid(
  TAF2_lfar$full_plot,
  plot_grid(
    get_legend(TAF2_lfar$for_external_legend),
    get_legend(TAF2_lfar$for_internal_legend),
    nrow = 1
  ),
  nrow = 2,
  rel_heights = c(7,3)
)

g_lfar
```

Adding the LFAR definitely improves the LFAR assignment, though
it still does not get it perfect.
```{r}
ggsave(g_lfar, filename = output_list$assig_table_lfar_all, width = 10, height = 12)
```



# Calculating Fst between the collections from the microhaplotype markers

We use the 'hierfstat' package to get the pairwise
$F_\mathrm{ST}$ values (Weir & Cockerham method) between the collections from these microhaplotype data.

In order to do this we need to convert the data to a lumped format.
This means making integers of the alleles and then catenating them.
One locus has 11 allele, so we should use two digits for each allele.  We drop the
LFAR from this, because those were chosen specifically to be weird in the late fall.
```{r}
full_base_long <- full_base2 %>% 
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  mutate(
    Pop = as.integer(factor(collection, levels = collection_order)),
    .before = collection
  ) %>%
  select(-repunit, -collection, -sample_type) %>%
  pivot_longer(
    cols = c(-indiv, -Pop),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  )

hfs_ref_long <- full_base_long %>%
  group_by(locus) %>%
  mutate(
    alle_int =  sprintf("%02d", as.integer(factor(value)))
  ) %>%
  ungroup() %>%
  select(-value)

# then we can lump the alleles 
hfs_ref_wide <- hfs_ref_long %>%
  group_by(indiv, Pop, locus) %>%
  summarise(geno = str_c(alle_int[1], alle_int[2])) %>%
  ungroup() %>%
  mutate(geno = ifelse(str_detect(geno, "NA"), NA_character_, geno)) %>%
  mutate(geno = as.integer(geno)) %>%
  pivot_wider(
    names_from = c(locus),
    values_from = geno
  ) %>%
  arrange(Pop, indiv)

hfstat_dat <- hfs_ref_wide %>% 
  select(-indiv) %>%
  as.data.frame()


FST <- pp.fst(dat = hfstat_dat, diploid = TRUE)



# now, get the values out of it and put them in the upper diagonal
# of a matrix for plotting with the WGS values.
fst_mat <- FST$fst.pp 
colnames(fst_mat) <- collection_order
```


```{r}
  
all_values_tib <- fst_mat %>%
  as_tibble() %>%
  mutate(row_label = collection_order, .before = SRW) %>%
  pivot_longer(
    -row_label, 
    names_to = "col_label",
    values_to = "fst"
  )

# to get it to the right format we make it look like rubias
# output and the pipe it into taf_prep()
upper_tri_fst <- all_values_tib %>%
  filter(!is.nan(fst)) %>%
  mutate(
    collection_f = factor(row_label, levels = collection_order),
    inferred_coll_f = factor(col_label, levels = collection_order)
  ) %>% 
  select(-row_label, -col_label) %>%
  mutate(n = sprintf("%0.3f", fst)) %>%
  select(-fst) %>%
  taf_prep()
  
 
```


Now that we have the `upper_tri_fst` values in the right format, we just need
to add values to the lower triangle part of the matrix.  


# Adding allele frequency scatter plots to the lower diagonal

I thought about doing differentiation tests, or histograms of per-locus
Fst values, but then I realized that it actually might be way more informative
to put small-multiple scatter plots of allele frequencies in each of the lower-diagonal
cells of the Fst figure.  I think it might look cool.

We will also standardize each sample size (at each locus) to the minimum
number of observed genotypes in a population.


## Calculate allele frequencies for each population and standardize to lowest sample size

Straightforward tidyverse stuff:
```{r}

# long format with no missing data
long_genos <- full_base2 %>%
  select(-repunit, -sample_type) %>%
  pivot_longer(
    cols = -c(indiv, collection),
    names_to = c("locus", "gene_copy"),
    values_to = "allele",
    names_pattern = "(.*)[_.]([12])$"
  ) %>%
  filter(!is.na(allele))

# now get the minimum sample sizes for each
min_sampled <- long_genos %>%
  count(collection, locus) %>%
  group_by(locus) %>%
  summarise(min_ngc = min(n))

# having looked at those, if we keep anything with
# at least 38 gene copies, we have most of the loci.
# So, we will use this to slice sample some individuals from
# each population. We have to do a bunch of rigamoral because
# slice_sample requires that the n argument is a constant.  BOGUS!
set.seed(5)
long_genos_subsampled <- long_genos %>%
  left_join(min_sampled, by = "locus") %>%
  filter(min_ngc >= 38) %>%
  group_by(collection, locus, min_ngc) %>%
  nest() %>%
  mutate(
    ssd = map2(.x = data, .y = min_ngc, .f = function(x, y) slice_sample(x, n = y))
  ) %>%
  select(-data) %>%
  unnest(ssd) %>%
  select(-min_ngc)

  

# get allele frequencies
alle_freqs <- long_genos_subsampled %>%
  count(collection, locus, allele) %>%
  group_by(collection, locus) %>%
  mutate(freq = n / sum(n)) %>%
  select(-n) %>%
  ungroup()


# get all pairwise comparisons of population allele frequencies
backbone <- expand_grid(
  row_coll =  unique(alle_freqs$collection),
  col_coll = unique(alle_freqs$collection),
  alle_freqs %>% distinct(locus, allele)
)

# now get the freqs on there, order the collections and ensure
# that we are on the diagonal and the lower triangle, and then also get
# the positions for each of the points.  We will allow that the whole
# scatterplot can be inset into the cell by a fractional amount inset
inset <- 0.15
pair_freqs <- backbone %>%
  left_join(alle_freqs, by = join_by(row_coll == collection, locus, allele)) %>%
  rename(row_freq = freq) %>%
  left_join(alle_freqs, by = join_by(col_coll == collection, locus, allele)) %>%
  rename(col_freq = freq) %>%
  mutate(
    row_coll = factor(row_coll, levels = collection_order),
    col_coll = factor(col_coll, levels = collection_order)
  ) %>%
  filter(as.integer(col_coll) <= as.integer(row_coll)) %>%
  replace_na(list(row_freq = 0.0, col_freq = 0.0)) %>%
  mutate( # this is just setting things as in table_as_figure
      ymin = nlevels(row_coll) - as.integer(row_coll) + inset,
      ymax = nlevels(row_coll) - as.integer(row_coll) + 1 - inset,
      xmin = as.integer(col_coll) - 1 + inset,
      xmax = as.integer(col_coll) - inset
    ) %>%
  mutate(  # here we get the x-y positions for the scatter points
    x = xmin + col_freq * (xmax - xmin),
    y = ymin + row_freq * (ymax - ymin)
  )

```

Making a figure.
```{r}

# we need to get colors and things for the fill.  We just take the
# rubias output that was run through taf_prep and we keep the lower
# diagonal and we set all the cell_labels to NA.
lower_tri_base <- Rubias_for_later %>%
  filter(as.integer(col_label) <= as.integer(row_label)) %>%
  mutate(cell_label = NA_character_)

# then we put those together and make a figure
full_fst <- bind_rows(
  upper_tri_fst,
  lower_tri_base
) %>%
  arrange(row_label, col_label)


# get the result
TAF_FST <- table_as_figure(
  X = full_fst, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors,
  Xs_on_diagonal = FALSE,
  plot_margins = c(0.7, 0.1, 0.1, 0.1)
)


# then plot it and add the scatter points on the lower diagonal
fst_with_scatters <- TAF_FST$full_plot +
  geom_point(
    data = pair_freqs,
    mapping = aes(x = x, y = y),
    size = 0.025,
    colour = "black"
  )


```

And we can save that too:
```{r}
ggsave(fst_with_scatters, file = output_list$fst_table, width = 6, height = 5.1)
```

# Put them together into a single figure for the paper

```{r}
gsi_fst_fig_for_paper <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  fst_with_scatters,
  nrow = 3,
  rel_heights = c(6,1.5,6)
)

# and then save that
ggsave(gsi_fst_fig_for_paper, file = output_list$gsi_fst_fig, width = 10, height = 19)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$gsi_fst_fig, collapse = " ")
system(CALL)

file.rename(from = output_list$gsi_fst_fig, to = output_list$gsi_fst_fig_tex)
```



# Population Genetic Summaries

We can do all of our standard population genetic summaries here using `full_base2`.

It is pretty standard, but I think that i am going to standardize sample sizes.

```{r}
fbl_no_miss <- full_base2 %>%
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  pivot_longer(
    cols = c(-(indiv:sample_type)),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  ) %>%
  group_by(indiv, locus) %>%
  mutate(isHz = value[1] != value[2]) %>%
  ungroup() %>%
  filter(!is.na(value) & !is.na(isHz))


# now we just summarize by collection and locus
simple_summaries <- fbl_no_miss %>%
  group_by(repunit, collection, locus) %>%
  summarise(
    num_diploids = n() / 2.0,
    obs_Hz = mean(isHz),
    num_alleles = n_distinct(value)
  ) %>%
  ungroup()



# here, we get the minimum number of diploids at each locus
min_gene_copies <- simple_summaries %>%
  group_by(locus) %>%
  summarise(min_gc = min(num_diploids) * 2)

# here is a function to take N samples of size s from a vector
# and return the mean number of distinct elements in it:
subsample_num_alleles <- function(v, s, N=1000) {
  samples <- lapply(
    1:N, 
    function(z) {
      sample(x = v, size = s, replace = FALSE) %>% n_distinct()
    }) %>% unlist()
  
    
    # now, return a tibble with columns mean_num_alle_subsamp an fract_polymorphic_subsamp
    tibble(
      mean_num_alle_subsamp = mean(samples),
      fract_polymorphic_subsamp = mean(samples > 1)
    )
}

# we will now use that min_diploid number to downsample
# each collection randomly, without replacement 1000 times and 
# calculate the mean number of alleles.
sub_sampled_locus_num_alle <- fbl_no_miss %>%
  left_join(min_gene_copies, by = join_by(locus)) %>%
  group_by(repunit, collection, locus) %>%
  summarise(
    mean_num_alleles = list(subsample_num_alleles(value, min_gc[1], N = 100))
  ) %>%
  ungroup() %>%
  unnest(mean_num_alleles)

# and now we can calculate the means over loci of those
subby <- sub_sampled_locus_num_alle %>%
  group_by(repunit, collection) %>%
  summarise(
    mean_num_alle = mean(mean_num_alle_subsamp),
    mean_fract_poly = mean(fract_polymorphic_subsamp)
  )

# here we will get the estimated allele freqs and, from those, calculate
# expected heterozygosity as 1 - \sum_alleles freq^2
expHz <- fbl_no_miss %>%
  group_by(repunit, collection, locus, value) %>%
  summarise(
    n = n()
  ) %>%
  mutate(freq = n / sum(n)) %>%
  group_by(repunit, collection, locus) %>%
  summarise(exp_Hz = 1 - sum(freq ^2)) %>%
  ungroup()


# now, join that on there:
locus_summaries <- simple_summaries %>%
  left_join(expHz, by = join_by(repunit, collection, locus))

# and now we can summarise there into means and SDs
mean_Hzs_etc <- locus_summaries %>%
  group_by(repunit, collection) %>%
  summarise(
    mean_num_dip = mean(num_diploids),
    mean_expHz = mean(exp_Hz),
    mean_obsHz = mean(obs_Hz),
  )

# OK, join the subsampled numbers onto those
tibble_for_paper <- subby %>%
  left_join(mean_Hzs_etc, by = join_by(repunit, collection)) %>%
  mutate(collection = factor(collection, levels = unique(pop_labels$collection))) %>%
  arrange(collection) %>%
  ungroup() %>%
  select(collection, mean_num_dip, mean_num_alle, mean_fract_poly, mean_expHz, mean_obsHz) %>%
  mutate(
    collection = as.character(collection),
    mean_num_dip = sprintf("%.1f", mean_num_dip),
    mean_num_alle = sprintf("%.2f", mean_num_alle),
    mean_fract_poly = sprintf("%.2f", mean_fract_poly),
    mean_expHz = sprintf("%.3f", mean_expHz),
    mean_obsHz = sprintf("%.3f", mean_obsHz)
  ) %>%
  rename(
    Code = collection,
    `$\\bar{N}$` = mean_num_dip,
    `$\\bar{N}_\\mathrm{A,ss}$` = mean_num_alle,
    `$\\bar{P}_\\mathrm{poly,ss}$` = mean_fract_poly,
    `$\\bar{\\mathrm{H}}_\\mathrm{exp}$` = mean_expHz,
    `$\\bar{\\mathrm{H}}_\\mathrm{obs}$` = mean_obsHz,
  )

tibble_for_paper$Code[1] <- sprintf("\\hline %s", tibble_for_paper$Code[1])

# then write that table out:
write_delim(
  tibble_for_paper,
  delim = "&",
  file = "tex/inputs/popgen-summary.tex", 
  eol = "\\tabularnewline\n", 
  quote = "none"
)
```


# Finally, get the histogram of the total number of alleles per locus


```{r}
numa <- fbl_no_miss %>%
  group_by(locus) %>%
  summarise(
    num_alle = n_distinct(value)
  ) %>%
  count(num_alle) %>%
  bind_rows(tibble(num_alle = 9, n = 0))
  

gbp <- ggplot(numa, aes(x = factor(num_alle), y = n)) +
  geom_col(fill = "blue") +
  theme_bw() +
  xlab("Total number of alleles") +
  ylab("Number of loci")

ggsave(gbp, filename = "tex/images/num-alle-barplot.pdf", width = 4, height = 3)

numa
```
