---
title: "Assessing Power of the California Microhap Baseline for GSI (and calculating Fst)"
author: "Eric C. Anderson"
date: "Last Updated: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_knit$set(root.dir = here::here())
start_time <- Sys.time()

USING_SNAKEMAKE <- FALSE
NEG_USING_SMK <- TRUE
if(exists("snakemake")) {
  USING_SNAKEMAKE <- TRUE
  NEG_USING_SMK <- FALSE
}
```

```{r rmd_snakemake_hacks, include=FALSE, eval=USING_SNAKEMAKE}
# currently, this block does not get evaluated, while developing,
# but later when we snakemake-ize it all, we will evaluate it.
if(USING_SNAKEMAKE) {
  input_list <- snakemake@input
  output_list <- snakemake@output
}
```

# Input and Output Paths

```{r input_bypass, include=NEG_USING_SMK, eval=NEG_USING_SMK}
# inputs:
input_list <- list(
  final_baseline = "data/cali-chinook-baseline.rds",
  pop_labels = "inputs/reference-collection-names.csv",
  map_notations = "inputs/map-notations.tsv"
)
# outputs:
output_list <- list(
  assig_table = "results/GSI_and_Fst/assignment-table-full-baseline.pdf",
  fst_table = "results/GSI_and_Fst/full-baseline-but-no-lfar-Fst-table.pdf",
  assig_table_lfar_all = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar.pdf",
  assig_table_lfar_no_miss = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar-and-everyone-has-the-lfar.pdf",
  tex_table = "tex/inputs/samples-table.tex",
  gsi_fst_fig = "results/GSI_and_Fst/gsi_and_fst_fig.pdf",
  gsi_fst_fig_crop = "results/GSI_and_Fst/gsi_and_fst_fig-crop.pdf",
  gsi_fst_fig_tex = "tex/images/gsi_and_fst_fig-crop.pdf"
)
# you can create the necessary output directories like this:
dump <- lapply(output_list, function(x)
  dir.create(dirname(x), recursive = TRUE, showWarnings = FALSE)
)
```


# Introduction

Here we assess power of the baseline for population assignment

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rubias)
library(cowplot)
library(hierfstat)
library(lubridate)


# get the full baseline with the new markers, etc.  We will toss out some
# non-winter Sacto fish of unknown provenance
full_base <- read_rds(input_list$final_baseline) %>%
  filter(collection != "SacNonW")


# read the pop labels for proper sorting
pop_labels <- read_csv(input_list$pop_labels)

# rename those collections and repunits
full_base2 <- full_base %>%
  rename(old_name = collection) %>%
  select(-repunit) %>%
  left_join(pop_labels %>% select(-run_timing_group), by = join_by(old_name)) %>%
  select(-old_name) %>%
  select(indiv, repunit, collection, sample_type, everything())
  
# finally, get the order of collections and repunits we want
collection_order <- unique(pop_labels$collection)
```


# Make a table summary of all the samples

I would like to break things down by general sampling location, run type, number of samples,
month-range of sampling, and range of years.
```{r}
meta <- read_csv("data/baseline_repository_meta_complete.csv.gz") %>%
  rename(indiv = NMFS_DNA_ID...1) %>%
  select(
    indiv, REPORTED_LIFE_STAGE, PHENOTYPE, COLLECTION_DATE, ESTIMATED_DATE, WATERSHED, TRIB_1,
    TRIB_2, WATER_NAME, REACH_SITE, HATCHERY, LATITUDE_F,
    LONGITUDE_F, LOCATION_COMMENTS_F
  ) %>%
  mutate(
    collection_date = mdy(COLLECTION_DATE),
    month = month(collection_date),
    year = year(collection_date)
  )


# get the notations on the map, and, in particular get the distinct values of
# abbrv and name_text
map_notations <- read_tsv(input_list$map_notations)

map_notations_dist1 <- map_notations %>%
  distinct(abbrv, repository_name, map_text, run_timing, run_timing_for_table)


# join the baseline to the meta data and standardize names and nest everything on abbrv and repository name
fbmet <- full_base2 %>%
  select(1:4) %>%
  left_join(meta, by = join_by(indiv)) %>%
  rename(abbrv = collection) %>%
  mutate(
    repository_name = case_when(
      !is.na(HATCHERY) ~ HATCHERY,
      TRUE ~ WATER_NAME
    )
  ) %>%
  left_join(map_notations_dist1, by = join_by(abbrv, repository_name)) %>%
  group_by(abbrv, repository_name, run_timing_for_table, map_text, WATERSHED) %>%
  nest()


# then extract bits of information from the data and set sparkline paths
fbmet2 <- fbmet %>%
  mutate(
    N = map_int(.x = data, .f = function(x) nrow(x)),
    `Run Timing` = map_chr(.x = data, .f = function(x) x$run_timing[1]),
    min_year = map_int(.x = data, .f = function(x) base::min(x$year, na.rm = TRUE)),
    max_year = map_int(.x = data, .f = function(x) base::max(x$year, na.rm = TRUE)),
    sparkline_path = str_c("tex/images/months-", abbrv, "_", str_replace_all(repository_name, " ", "-"), ".pdf")
  )


# then make the images.  Put them right into the tex directory
source("R/month_sample_sparkline.R")

dir.create("tex/images", recursive = TRUE, showWarnings = FALSE)

fbmet3 <- fbmet2 %>%
  mutate(
    ggplot = map2(.x = data, .y = sparkline_path, .f = function(x, y) month_sample_sparkline(tib = x, path = y))
  )

# now, add all the columns we might need.
fbmet4 <- fbmet3 %>%
  left_join(map_notations %>% select(abbrv, repository_name, map_text, basin, ESU, `Mainstem River` ))

# and now do some formatting
fbmet5 <- fbmet4 %>%
  ungroup() %>%
  mutate(
    `Sampling Months` = sprintf("\\raisebox{-0.12 em}{\\includegraphics[height=1.02em]{../%s}}", sparkline_path),
    `Year Range` = sprintf("%d--%d", min_year, max_year)
  ) %>%
  select(`Mainstem River`, abbrv, map_text, run_timing_for_table, ESU, N, `Sampling Months`, `Year Range`) %>%
  rename(
    `Mainstem River$^1$` = `Mainstem River`,
    Code = abbrv,
    `Location Name$^2$` = map_text,
    Run = run_timing_for_table,
    `ESU$^3$` = ESU,
    `$N$` = N
  )

# a hack to get an hline under the heading:
fbmet5$`Mainstem River$^1$`[1] <- str_c("\\hline ", fbmet5$`Mainstem River$^1$`[1])

# Finally write out the table
write_delim(fbmet5, delim = "&", eol = "\\tabularnewline\n", file = output_list$tex_table)



```




# Do the self-assignment

In this round, we go directly to assigning to repunit, and then to the max-likelihood
collection within that repunit.

```{r}
full_sa <- self_assign(full_base2, gen_start_col = 5)
```

```{r}
top_ass <- full_sa %>%
  group_by(indiv, collection, repunit, inferred_repunit) %>%
  mutate(repu_sclike = sum(scaled_likelihood)) %>%
  group_by(indiv) %>%
  filter(repu_sclike == max(repu_sclike)) %>%
  arrange(indiv, desc(scaled_likelihood)) %>%
  slice(1) %>%
  ungroup() %>%
  mutate(coll_f = factor(collection, levels = collection_order))
```

And, once again, we want to investigate the ones that are in the wrong repunit, and look at their
scaled_repu_likelihoods, the others.
```{r}
source("R/colors.R")
ta_corr <- top_ass %>%
  filter(repunit == inferred_repunit)
ta_wrong <- top_ass %>%
  filter(repunit != inferred_repunit) %>%
  arrange(collection, repu_sclike) %>%
  group_by(collection) %>%
  mutate(y = 190 + (70/9) * 1:n())

ggplot() +
  geom_histogram(
    data = ta_corr, 
    mapping = aes(x = repu_sclike, fill = inferred_repunit)
  ) +
  geom_point(
    data = ta_wrong,
    mapping = aes(x = repu_sclike, y = y, fill = inferred_repunit),
    size = 3, 
    shape = 21
  ) +
  facet_wrap(~coll_f, ncol = 3) +
  scale_fill_manual(values = repunit_colors)
```
Which looks to me like there are really only 7 here that we are not getting right.
Those others we will filter out and make a table of them:

```{r}
tafilt <- top_ass %>%
  filter(!(repunit != inferred_repunit & repu_sclike > 0.99))
```

Here we calculate the number of correctly and incorrectly assigned fish:
```{r}
tafilt %>%
  mutate(correct = repunit == inferred_repunit) %>%
  count(correct) %>%
  mutate(fract = n / sum(n))
```

And now we will make a table of those:
```{r}
# get the function
source("R/taf-prep.R")
source("R/table-as-figure.R")

# get the RC_groups
RC_groups <- pop_labels %>%
  select(-old_name) %>%
  distinct() %>%
  rename(external_group = run_timing_group, internal_group = repunit, label = collection) %>%
  select(external_group, internal_group, label)

X2 <- tafilt %>%
  mutate(
    collection_f = factor(collection, levels = collection_order),
    inferred_coll_f = factor(inferred_collection, levels = collection_order)
  ) %>%
  count(collection_f, inferred_coll_f, .drop = FALSE) %>%
  taf_prep()

# get the result
TAF2 <- table_as_figure(
  X = X2, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors, 
  plot_margins = c(1, 0.1, 0.7, 0.1)
)

# and plot it
g <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  nrow = 2,
  rel_heights = c(7,3)
)

g
```

That is a considerable improvement. Let's write it out:
```{r}
ggsave(g, filename = output_list$assig_table, width = 8, height = 10)
```


# Calculating Fst between the collections from the microhaplotype markers

We use the 'hierfstat' package to get the pairwise
$F_\mathrm{ST}$ values (Weir & Cockerham method) between the collections from these microhaplotype data.

In order to do this we need to convert the data to a lumped format.
This means making integers of the alleles and then catenating them.
One locus has 11 allele, so we should use two digits for each allele.  We drop the
LFAR from this, because those were chosen specifically to be weird in the late fall.
```{r, cache=TRUE}
full_base_long <- full_base2 %>% 
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  mutate(
    Pop = as.integer(factor(collection, levels = collection_order)),
    .before = collection
  ) %>%
  select(-repunit, -collection, -sample_type) %>%
  pivot_longer(
    cols = c(-indiv, -Pop),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  )

hfs_ref_long <- full_base_long %>%
  group_by(locus) %>%
  mutate(
    alle_int =  sprintf("%02d", as.integer(factor(value)))
  ) %>%
  ungroup() %>%
  select(-value)

# then we can lump the alleles 
hfs_ref_wide <- hfs_ref_long %>%
  group_by(indiv, Pop, locus) %>%
  summarise(geno = str_c(alle_int[1], alle_int[2])) %>%
  ungroup() %>%
  mutate(geno = ifelse(str_detect(geno, "NA"), NA_character_, geno)) %>%
  mutate(geno = as.integer(geno)) %>%
  pivot_wider(
    names_from = c(locus),
    values_from = geno
  ) %>%
  arrange(Pop, indiv)

hfstat_dat <- hfs_ref_wide %>% 
  select(-indiv) %>%
  as.data.frame()


FST <- pp.fst(dat = hfstat_dat, diploid = TRUE)

# let's also bootstrap that:
FST_boot <- boot.ppfst(
  dat = hfstat_dat,
  nboot = 1000,
  quant = c(0.025,0.975),
  diploid = TRUE
)


# now, get the values out of it and put them in the upper diagonal
# of a matrix for plotting with the WGS values.
fst_mat <- FST$fst.pp 
colnames(fst_mat) <- collection_order
```


```{r}
  
all_values_tib <- fst_mat %>%
  as_tibble() %>%
  mutate(row_label = collection_order, .before = SRW) %>%
  pivot_longer(
    -row_label, 
    names_to = "col_label",
    values_to = "fst"
  )

# to get it to the right format we make it look like rubias
# output and the pipe it into taf_prep()
upper_tri_fst <- all_values_tib %>%
  filter(!is.nan(fst)) %>%
  mutate(
    collection_f = factor(row_label, levels = collection_order),
    inferred_coll_f = factor(col_label, levels = collection_order)
  ) %>% 
  select(-row_label, -col_label) %>%
  mutate(n = sprintf("%0.3f", fst)) %>%
  select(-fst) %>%
  taf_prep()
  
 
```


Now that we have the `upper_tri_fst` values in the right format, we just need
to add the WGS values to the lower triangle part of the matrix.  


# Adding the WGS Fst values and making a figure

```{r}
# for now I am just going to put the WGS Fst's into the lower diagonal, but I think it will be better, ultimately,
# to but the bootstrapped CI's in there.
wgs_mat <- read_rds("../anderson-et-al-chinook-ecotypic-genetic-architecture/outputs/001/full-wgs-fst-mat.rds")

# once again, make it looks like rubias output and taf_prep() it.
lower_tri_diag <- wgs_mat %>%
  mutate(
    collection_f = factor(pop1_f, levels = collection_order),
    inferred_coll_f = factor(pop2_f, levels = collection_order),
    n = fst
  ) %>%
  select(collection_f, inferred_coll_f, n) %>%
  complete(collection_f, inferred_coll_f) %>%
  filter(as.integer(collection_f) >= as.integer(inferred_coll_f)) %>%  # this gets lower tri + diagonal
  taf_prep()
  

# then we put those together and make a figure
full_fst <- bind_rows(
  upper_tri_fst,
  lower_tri_diag
) %>%
  arrange(row_label, col_label)


# get the result
TAF_FST <- table_as_figure(
  X = full_fst, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors,
  Xs_on_diagonal = TRUE,
  plot_margins = c(0.7, 0.1, 0.1, 0.1)
)


# and plot it
TAF_FST$full_plot


```

And we can save that too:
```{r}
ggsave(TAF_FST$full_plot, file = output_list$fst_table, width = 6, height = 5.1)
```

# Put them together into a single figure for the paper

```{r}
gsi_fst_fig_for_paper <- plot_grid(
  TAF2$full_plot,
  plot_grid(
    get_legend(TAF2$for_external_legend),
    get_legend(TAF2$for_internal_legend),
    nrow = 1
  ),
  TAF_FST$full_plot,
  nrow = 3,
  rel_heights = c(6,1.5,6)
)

# and then save that
ggsave(gsi_fst_fig_for_paper, file = output_list$gsi_fst_fig, width = 7, height = 14)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$gsi_fst_fig, collapse = " ")
system(CALL)

file.rename(from = output_list$gsi_fst_fig, to = output_list$gsi_fst_fig_tex)
```
