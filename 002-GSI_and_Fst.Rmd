---
title: "Assessing Power of the California Microhap Baseline for GSI (and calculating Fst)"
author: "Eric C. Anderson"
date: "Last Updated: `r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
start_time <- Sys.time()
```

# Input and Output Paths

```{r}
if(exists("snakemake")) {
  input_list <- snakemake@input
  output_list <- snakemake@output
} else {
  input_list <- list(
    final_baseline = "data/SWFSC-chinook-reference-baseline.csv.gz",
    pop_labels = "inputs/reference-collection-names.csv",
    map_notations = "inputs/map-notations.tsv"
  )
  # outputs:
  output_list <- list(
    tex_table = "tex/inputs/samples-table.tex",
    gsi_fst_fig_tex = "tex/images/gsi_and_fst_fig-crop.pdf",
    ass_table80_tex = "tex/images/ass-table-80-crop.pdf",
    rosa_gsi_table_tex = "tex/images/rosa-gsi-table-crop.pdf",
    pop_gen_by_loc_coll = "tex/supp_data/Supp-Data-2-pop-gen-summaries-by-locus-and-collection.csv",
    popgen_summ = "tex/inputs/popgen-summary.tex",
    num_alle_barplot = "tex/images/num-alle-barplot.pdf"
  )
}

# then add the intermediates to it:
# then add the intermediate things to it
output_list <- c(
  output_list,
  assig_table = "results/GSI_and_Fst/assignment-table-full-baseline.pdf",
  fst_table = "results/GSI_and_Fst/full-baseline-but-no-lfar-Fst-table.pdf",
  assig_table_lfar_all = "results/GSI_and_Fst/assignment-table-full-baseline-plus-lfar.pdf",
  ass_table80 = "results/GSI_and_Fst/ass-table-80.pdf",
  rosa_gsi_table = "results/GSI_and_Fst/rosa-gsi-table.pdf",
  gsi_fst_fig = "results/GSI_and_Fst/gsi_and_fst_fig.pdf",
  gsi_fst_fig_crop = "results/GSI_and_Fst/gsi_and_fst_fig-crop.pdf",
  ass_table80_crop = "results/GSI_and_Fst/ass-table-80-crop.pdf",
  rosa_gsi_table_crop = "results/GSI_and_Fst/rosa-gsi-table-crop.pdf"
)

    
# you can create the necessary output directories like this:
dump <- lapply(output_list, function(x)
  dir.create(dirname(x), recursive = TRUE, showWarnings = FALSE)
)
```


# Introduction

Here we assess power of the baseline for population assignment

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(rubias)
library(cowplot)
library(hierfstat)
library(lubridate)


# read in the full baseline
full_base <- read_csv(input_list$final_baseline)


# capture that rosa hapstr with some meta data from future use
rosa_genos0 <- full_base %>%
  select(indiv:rosa_genotypes_str)



# read the pop labels for proper sorting
pop_labels <- read_csv(input_list$pop_labels)

# rename those collections and repunits
full_base2 <- full_base %>%
  rename(old_name = collection) %>%
  select(-repunit) %>%
  left_join(pop_labels %>% select(old_name, collection, repunit), by = join_by(old_name)) %>%
  select(-old_name) %>%
  select(indiv, repunit, collection, sample_type, everything())


# finally, get the order of collections and repunits we want
collection_order <- unique(pop_labels$collection)


```


# Make a table summary of all the samples

I would like to break things down by general sampling location, run type, number of samples,
month-range of sampling, and range of years.
```{r}
meta <- read_csv("data/baseline_repository_meta_complete.csv.gz") %>%
  rename(indiv = NMFS_DNA_ID...1) %>%
  select(
    indiv, REPORTED_LIFE_STAGE, PHENOTYPE, COLLECTION_DATE, ESTIMATED_DATE, WATERSHED, TRIB_1,
    TRIB_2, WATER_NAME, REACH_SITE, HATCHERY, LATITUDE_F,
    LONGITUDE_F, LOCATION_COMMENTS_F
  ) %>%
  mutate(
    collection_date = mdy(COLLECTION_DATE),
    month = month(collection_date),
    year = year(collection_date)
  )


# get the notations on the map, and, in particular get the distinct values of
# abbrv and name_text
map_notations <- read_tsv(input_list$map_notations)

map_notations_dist1 <- map_notations %>%
  distinct(abbrv, repository_name, map_text, run_timing, run_timing_for_table)


# join the baseline to the meta data and standardize names and nest everything on abbrv and repository name
fbmet <- full_base2 %>%
  select(1:4) %>%
  left_join(meta, by = join_by(indiv)) %>%
  rename(abbrv = collection) %>%
  mutate(
    repository_name = case_when(
      !is.na(HATCHERY) ~ HATCHERY,
      TRUE ~ WATER_NAME
    )
  ) %>%
  left_join(map_notations_dist1, by = join_by(abbrv, repository_name)) %>%
  group_by(abbrv, repository_name, run_timing_for_table, map_text, WATERSHED) %>%
  nest()

# then extract bits of information from the data and set sparkline paths
fbmet2 <- fbmet %>%
  mutate(
    N = map_int(.x = data, .f = function(x) nrow(x)),
    N_date = map_int(.x = data, .f = function(x) sum(!(is.na(x$collection_date)) & yday(x$collection_date) != 1) ),  # we use YYYY-01-01 to denote year is known but day is missing in the data repository, so here we filter those out.
    `Run Timing` = map_chr(.x = data, .f = function(x) x$run_timing[1]),
    min_year = map_int(.x = data, .f = function(x) base::min(x$year, na.rm = TRUE)),
    max_year = map_int(.x = data, .f = function(x) base::max(x$year, na.rm = TRUE)),
    sparkline_path = str_c("tex/images/months-", abbrv, "_", str_replace_all(repository_name, " ", "-"), ".pdf")
  )


# then make the images.  Put them right into the tex directory
source("R/month_sample_sparkline.R")

dir.create("tex/images", recursive = TRUE, showWarnings = FALSE)

fbmet3 <- fbmet2 %>%
  mutate(
    ggplot = map2(.x = data, .y = sparkline_path, .f = function(x, y) month_sample_sparkline(tib = x, path = y))
  )

# now, add all the columns we might need.
fbmet4 <- fbmet3 %>%
  left_join(map_notations %>% select(abbrv, repository_name, map_text, basin, ESU, `Mainstem River` )) %>%
  left_join(pop_labels %>% select(collection, repunit), by = join_by(abbrv == collection))

# and now do some formatting
fbmet5 <- fbmet4 %>%
  ungroup() %>%
  mutate(
    `Sampling Months` = sprintf("\\raisebox{-0.12 em}{\\includegraphics[height=1.02em]{../%s}}", sparkline_path),
    `Year Range` = sprintf("%d--%d", min_year, max_year)
  ) %>%
  select(`Mainstem River`, abbrv, map_text, run_timing_for_table, repunit, ESU, N, N_date, `Sampling Months`, `Year Range`) %>%
  mutate(map_text = str_replace(map_text, ",.*$", "")) %>% # this removes the abbreviations after the commas on the map names
  mutate(N_date = replace_na(N_date, 0L)) %>%
  rename(
    `Mainstem River$^1$` = `Mainstem River`,
    Code = abbrv,
    `Location Name$^2$` = map_text,
    Run = run_timing_for_table,
    `Reporting Unit$^3$` = repunit,
    `ESU$^4$` = ESU,
    `$N$` = N,
    `$N_\\mathrm{day}^5$` = N_date
  ) %>%
  arrange(factor(Code, levels = collection_order))

# a hack to get an hline under the heading:
fbmet5$`Mainstem River$^1$`[1] <- str_c("\\hline ", fbmet5$`Mainstem River$^1$`[1])

# Finally write out the table
write_delim(fbmet5, delim = "&", eol = "\\tabularnewline\n", file = output_list$tex_table)



```


## Get a quick summary of number of collections and samples by ecotype

One of the reviewers wanted summaries in the text.  That's a great idea.
Here we get the numbers that we need.  
```{r}
# total fish:
sum(fbmet4$N)
```

```{r}
# numbers of samples and collection by ecotype
fbmet4 %>%
  group_by(`Run Timing`) %>%
  summarise(
    num_collections = n(),
    num_samples = sum(N)
  )
```

# Let's also just do a summary of the number of markers of different kinds

```{r}
# get the distinct loci in it
loci_tib <- read_csv("tex/supp_data/Supp-Data-1-Amplicon-info.csv")

# now, pull out their specifiers and count
loc_table <- loci_tib %>%
  mutate(
    spec = str_sub(AmpliconName, 5, 8),
    num_snps = str_count(SNPLocationsInAmpliconSequence, ",") + 1L
  ) %>%
  group_by(spec) %>%
  summarise(num_amplicons = n(), num_variants = sum(num_snps)) %>%
  arrange(desc(num_amplicons)) %>%
  rename(
    Type = spec,
    `N (amplicons)` = num_amplicons,
    `N (variants)` = num_variants
  )

# just write this out and then stick it in the doc
write.table(loc_table, sep = "&", na = "", quote = FALSE, row.names = FALSE)
```

# Get a version with no LFAR

We might want to do a run with no LFAR in there just to see how much
they add to the power for identifying the late fall.


```{r}

# get the positions of the LFARs:
lfar_idxs <- which(str_detect(names(full_base2), "lfar"))

# then make a version of the baseline without them
base_with_no_lfar <- full_base2[, -lfar_idxs]
```


# Process the RoSA hapstrs to include at some point

We are not going to delve on the difference between E1 and E2 here.  That is for the
subRoSA paper.  But we want to get the genotypes simply as EE, EL, LL as done in the
science paper.  

Let us have a quick look at the hapstrs:
```{r}
rosa_genos0 %>%
  count(rosa_genotypes_str) %>%
  arrange(desc(n))
```

We will explode those into a vector of characters at positions
`c(1, 3, 4, 6:10)`, thus dropping the E2-specific variants as
well as the Tasha2 SNP that effectively tags the duplication, leaving
us with the 8 core RoSA SNPs.
```{r}
H8 <- rosa_genos0 %>%
  select(indiv, rosa_genotypes_str) %>%
  mutate(
    hvec = map(rosa_genotypes_str, function(x) {y <- strsplit(x, "")[[1]]; y[c(1, 4, 5, 7:11)]}),
    hapstr8 = map_chr(hvec, function(x) paste(x, collapse = "", sep = ""))
  )

# Let's count up those hapstr8's
H8 %>%
  count(hapstr8) %>%
  arrange(desc(n))
```

From that it is clear that there have been some recombinations in here. But there is a bit of
junk in here.  It looks to me like there are some sites that get N's and some that get ?'s.  Both
of those are missing data.  I am going to be stringent here and toss anything with missing data,
which is only about 71 of them, so that should be fine. It will make downstream processing
much easier, since the number of categories will be a bit smaller. (And we can be quite confident
in these assignments)
```{r}
H8_nomiss <- H8 %>%
  filter(str_count(hapstr8, pattern = "\\?|N") == 0)
```

How many were filtered out there?
```{r}
nrow(H8) - nrow(H8_nomiss)
```

Let's look at them:
```{r, df.print = 20}
H8_nomiss %>%
  count(hapstr8) %>%
  arrange(desc(n))
```



```{r}
H8_monolith <- H8_nomiss %>%
  filter(hapstr8 %in% c("LLLLLLLL", "EEEEEEEE", "HHHHHHHH"))

H8_recombs <- H8_nomiss %>%
  filter(!(hapstr8 %in% c("LLLLLLLL", "EEEEEEEE", "HHHHHHHH")))

# call genos from H8_monolith
H8_genos <- H8_monolith %>%
  mutate(
    rosa_geno = recode(
      hapstr8,
      LLLLLLLL = "LL",
      EEEEEEEE = "EE",
      HHHHHHHH = "EL"
    )
  )
```



Now we will slim it down for future use:
```{r}
rosa_genos <- H8_genos %>%
  select(indiv, rosa_geno)
```

## Let's do a quick summary of the apparent recombinants

```{r, rows.print=30}
H8_recombs %>%
  left_join(
    full_base2 %>% select(indiv, repunit, collection),
    by = join_by(indiv)
  ) %>% 
  arrange(hapstr8)
```

# Do the self-assignment

We go directly to assigning to repunit, and then to the max-likelihood
collection within that repunit.  We don't include the lfar in here, at first.

```{r}
# gen_start_col is 6 because we have the rosa_genotypes_str column in there
full_sa <- self_assign(base_with_no_lfar, gen_start_col = 6)
```

```{r}
source("R/get_top_assignments.R")

top_ass <- get_top_assignments(full_sa, collection_order)
```

Plot that initial table:
```{r}
source("R/plot_self_assigment_table.R")

no_lfar_ass_table <- plot_self_assignment_table(top_ass$top, pop_labels, collection_order)

no_lfar_ass_table_80 <- top_ass$top %>% 
  filter(repu_sclike > 0.8) %>%
  plot_self_assignment_table(pop_labels, collection_order)

no_lfar_ass_table_Rosa <- top_ass$top %>%
  left_join(rosa_genos, by = join_by(indiv)) %>%
  plot_self_assignment_table(pop_labels, collection_order)
```


And then do the same for the data with the LFAR:
```{r}
lfar_sa <- self_assign(full_base2, gen_start_col = 6)
lfar_top_ass <- get_top_assignments(lfar_sa, collection_order)
lfar_ass_table <- plot_self_assignment_table(lfar_top_ass$top, pop_labels, collection_order)

# make a version with no legend on it
lfar_ass_table_no_legend <- plot_self_assignment_table(lfar_top_ass$top, pop_labels, collection_order, no_legend = TRUE)

lfar_ass_table_80 <- lfar_top_ass$top %>% 
  filter(repu_sclike > 0.8) %>%
  plot_self_assignment_table(pop_labels, collection_order)

lfar_ass_table_Rosa <- lfar_top_ass$top %>%
  left_join(rosa_genos, by = join_by(indiv)) %>%
  plot_self_assignment_table(pop_labels, collection_order)
```


## Get some raw numbers

```{r}
# total number of fish
tot_fish <- nrow(lfar_top_ass$top)
tot_fish

# Get the number and fraction correct
lfar_top_ass$top %>%
  count(repunit == inferred_repunit) %>%
  mutate(fract = n/tot_fish)
```

Look at the distribution there:
```{r}
lfar_top_ass$top %>%
  filter(repunit != inferred_repunit) %>%
  count(collection, inferred_repunit)
```

Get some more values for the paper:

From CHLF to Fall
```{r}
lfar_top_ass$top %>%
  filter(collection == "CHLF") %>%
  count(repunit == inferred_repunit) %>%
  mutate(fract = n / sum(n))
  
```

From Fall to CHLF
```{r}
lfar_top_ass$top %>%
  filter(repunit == "CV-Fall") %>%
  count(repunit,inferred_repunit) %>%
  mutate(fract = n / sum(n))
  
```

```{r}
lfar_top_ass$top %>%
  filter(collection == "MDS") %>%
  count(repunit,inferred_repunit) %>%
  mutate(fract = n / sum(n))
  
```


#### Now, look at the >80% criterion.
```{r}
# first, the number and fraction of fish that are still assigned
lfar_top_ass$top %>% 
  mutate(gt80 = repu_sclike > 0.8) %>%
  count(gt80, repunit == inferred_repunit)
```

```{r}
lfar_top_80 <- lfar_top_ass$top %>% 
  filter(repu_sclike > 0.8)

lfar_top_80 %>%
  count(repunit == inferred_repunit) %>%
  mutate(fract = n / sum(n))
```

Drop the clear winter run strays
```{r}

lfar_top_80 %>%
  filter(!((inferred_collection == "SRW") & (inferred_repunit != repunit))) %>%
  count(repunit == inferred_repunit) %>%
  mutate(fract = n / sum(n))
```

And particularly look at misassignments to the CHLF and CV-Spring

From Fall to other repunits
```{r}
lfar_top_80 %>%
  filter(repunit == "CV-Fall") %>%
  count(repunit,inferred_repunit) %>%
  mutate(fract = n / sum(n))
  
```



## Make a Plot for scaled likelihood > 0.8

Make a plot for > 80%
```{r}


# and then save that
ggsave(lfar_ass_table_80$plot, file = output_list$ass_table80, width = 10, height = 10.5)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$ass_table80, collapse = " ")
system(CALL)

file.rename(from = output_list$ass_table80_crop, to = output_list$ass_table80_tex)
```


## Make a plot with numbers broken out by the RoSA genotype

Make a plot for the byRosa:
```{r}


# and then save that
ggsave(lfar_ass_table_Rosa$plot, file = output_list$rosa_gsi_table, width = 10, height = 12.5)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$rosa_gsi_table, collapse = " ")
system(CALL)

file.rename(from = output_list$rosa_gsi_table_crop, to = output_list$rosa_gsi_table_tex)
```

## Look at the distribution of the E2 alleles across collections

While we are at it, let's look at the distribution of the E2 alleles across
collections.
```{r}
# count up genos at each variant
marks <- paste("s", 1:12, sep = "")
exploded <- rosa_genos0 %>%
  separate(rosa_genotypes_str, into = paste("s", 0:12, sep = ""), sep = "") %>%
  select(-s0)
rh_counts <-  exploded %>%
  pivot_longer(cols = starts_with("s"), names_to = "variant", values_to = "geno") %>%
  mutate(variant_f = factor(variant, levels = marks)) %>%
  count(repunit, collection, variant_f, geno)

# look just at s2, s3, and s6
rh_counts %>%
  filter(variant_f %in% c("s2", "s3", "s6")) %>%
  filter(geno %in% c("H", "W"))
```

So, in California, there is only one more E2-het outside of the Sacto, and that is from TRHsp. 
Let's go back and find that:
```{r}
exploded %>% 
  filter(repunit == "klamathspring" & s6 == "H") %>%
  left_join(rosa_genos0, by = join_by(indiv))

```

However, at ColeRivers we have 12 instances of heterozygotes at the s6 E2 position. Let's count things up from
ColeRivers:
```{r}
rosa_genos0 %>% 
  filter(collection == "ColeRHsp") %>%
  count(rosa_genotypes_str)
```


# Calculating Fst between the collections from the microhaplotype markers

We use the 'hierfstat' package to get the pairwise
$F_\mathrm{ST}$ values (Weir & Cockerham method) between the collections from these microhaplotype data.

I include the LFAR in their because I want to have it.  We note this is not for
demographic inference or anything.  It is just a summary of the differentiation
of the markers...

In order to do this we need to convert the data to a lumped format.
This means making integers of the alleles and then catenating them.
One locus has 11 allele, so we should use two digits for each allele.
```{r}
full_base_long <-  full_base2 %>% 
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  mutate(
    Pop = as.integer(factor(collection, levels = collection_order)),
    .before = collection
  ) %>%
  select(-repunit, -collection, -sample_type) %>%
  pivot_longer(
    cols = c(-indiv, -Pop),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  )

hfs_ref_long <- full_base_long %>%
  group_by(locus) %>%
  mutate(
    alle_int =  sprintf("%02d", as.integer(factor(value)))
  ) %>%
  ungroup() %>%
  select(-value)

# then we can lump the alleles 
hfs_ref_wide <- hfs_ref_long %>%
  group_by(indiv, Pop, locus) %>%
  summarise(geno = str_c(alle_int[1], alle_int[2])) %>%
  ungroup() %>%
  mutate(geno = ifelse(str_detect(geno, "NA"), NA_character_, geno)) %>%
  mutate(geno = as.integer(geno)) %>%
  pivot_wider(
    names_from = c(locus),
    values_from = geno
  ) %>%
  arrange(Pop, indiv)

hfstat_dat <- hfs_ref_wide %>% 
  select(-indiv) %>%
  as.data.frame()


FST <- pp.fst(dat = hfstat_dat, diploid = TRUE)



# now, get the values out of it and put them in the upper diagonal
# of a matrix for plotting with the WGS values.
fst_mat <- FST$fst.pp 
colnames(fst_mat) <- collection_order
```

       
```{r}
       
all_values_tib <- fst_mat %>%
as_tibble() %>%
mutate(row_label = collection_order, .before = SRW) %>%
pivot_longer(
-row_label, 
names_to = "col_label",
values_to = "fst"
)

# to get it to the right format we make it look like rubias
# output and the pipe it into taf_prep()
upper_tri_fst <- all_values_tib %>%
filter(!is.nan(fst)) %>%
mutate(
collection_f = factor(row_label, levels = collection_order),
inferred_coll_f = factor(col_label, levels = collection_order)
) %>% 
select(-row_label, -col_label) %>%
mutate(n = sprintf("%0.3f", fst)) %>%
select(-fst) %>%
taf_prep()


```

Now that we have the `upper_tri_fst` values in the right format, we just need
to add values to the lower triangle part of the matrix.  
       
       
# Adding allele frequency scatter plots to the lower diagonal
       
I thought about doing differentiation tests, or histograms of per-locus
Fst values, but then I realized that it actually might be way more informative
to put small-multiple scatter plots of allele frequencies in each of the lower-diagonal
cells of the Fst figure.  I think it might look cool.
       
We will also standardize each sample size (at each locus) to the minimum
number of observed genotypes in a population.
       
       
## Calculate allele frequencies for each population and standardize to lowest sample size
       
Straightforward tidyverse stuff:
```{r}

# long format with no missing data.  We filter out NA locus to get rid of the rosa_geno_str which
# we are not using
long_genos <- full_base2 %>%
  select(-repunit, -sample_type) %>%
  pivot_longer(
    cols = -c(indiv, collection),
    names_to = c("locus", "gene_copy"),
    values_to = "allele",
    names_pattern = "(.*)[_]([12])$"
  ) %>%
  filter(!is.na(allele) & !is.na(locus))

# now get the minimum sample sizes for each
min_sampled <- long_genos %>%
  count(collection, locus) %>%
  group_by(locus) %>%
  summarise(min_ngc = min(n))

# having looked at those, if we keep anything with
# at least 38 gene copies, we have most of the loci.
# So, we will use this to slice sample some individuals from
# each population. We have to do a bunch of rigamoral because
# slice_sample requires that the n argument is a constant.  BOGUS!
set.seed(5)
long_genos_subsampled <- long_genos %>%
  left_join(min_sampled, by = "locus") %>%
  filter(min_ngc >= 38) %>%
  group_by(collection, locus, min_ngc) %>%
  nest() %>%
  mutate(
    ssd = map2(.x = data, .y = min_ngc, .f = function(x, y) slice_sample(x, n = y))
  ) %>%
  select(-data) %>%
  unnest(ssd) %>%
  select(-min_ngc)



# get allele frequencies
alle_freqs <- long_genos_subsampled %>%
  count(collection, locus, allele) %>%
  group_by(collection, locus) %>%
  mutate(freq = n / sum(n)) %>%
  select(-n) %>%
  ungroup()


# get all pairwise comparisons of population allele frequencies
backbone <- expand_grid(
  row_coll =  unique(alle_freqs$collection),
  col_coll = unique(alle_freqs$collection),
  alle_freqs %>% distinct(locus, allele)
)

# now get the freqs on there, order the collections and ensure
# that we are on the diagonal and the lower triangle, and then also get
# the positions for each of the points.  We will allow that the whole
# scatterplot can be inset into the cell by a fractional amount inset
inset <- 0.15
pf_init <- backbone %>%
  left_join(alle_freqs, by = join_by(row_coll == collection, locus, allele)) %>%
  rename(row_freq = freq) %>%
  left_join(alle_freqs, by = join_by(col_coll == collection, locus, allele)) %>%
  rename(col_freq = freq) %>%
  mutate(
    row_coll = factor(row_coll, levels = collection_order),
    col_coll = factor(col_coll, levels = collection_order)
  ) %>%
  filter(as.integer(col_coll) <= as.integer(row_coll)) 

# now, here is a somewhat more complex step.  The problem is that some collections have
# no observations of any loci, it seems.  In those cases, we want the NA values to stand.
# Those cases can be recognized because every allele at a locus in them is NA. In cases,
# where not all of the alleles have NA for frequency, we do want to set the NAs to 0.
pf0s <- pf_init %>%
  group_by(row_coll, col_coll, locus) %>%
  mutate(
    allNA_rowfreq = all(is.na(row_freq)),
    allNA_colfreq = all(is.na(col_freq)),
  ) %>%
  ungroup() %>%
  mutate(
    row_freq = ifelse(is.na(row_freq) & !allNA_rowfreq, 0.0, row_freq),
    col_freq = ifelse(is.na(col_freq) & !allNA_colfreq, 0.0, col_freq)
  )


# finally, do some more setting up of things:
pair_freqs <- pf0s %>% 
  mutate( # this is just setting things as in table_as_figure
    ymin = nlevels(row_coll) - as.integer(row_coll) + inset,
    ymax = nlevels(row_coll) - as.integer(row_coll) + 1 - inset,
    xmin = as.integer(col_coll) - 1 + inset,
    xmax = as.integer(col_coll) - inset
  ) %>%
  mutate(  # here we get the x-y positions for the scatter points
    x = xmin + col_freq * (xmax - xmin),
    y = ymin + row_freq * (ymax - ymin)
  )

```

Making a figure.
```{r}

# we need to get colors and things for the fill.  We just take the
# rubias output that was run through taf_prep and we keep the lower
# diagonal and we set all the cell_labels to NA.
lower_tri_base <- lfar_ass_table$RFL %>%
  filter(as.integer(col_label) <= as.integer(row_label)) %>%
  mutate(cell_label = NA_character_)

# then we put those together and make a figure
full_fst <- bind_rows(
  upper_tri_fst,
  lower_tri_base
) %>%
  arrange(row_label, col_label)


# Get the RC_groups
RC_groups <- pop_labels %>%
  select(-old_name) %>%
  distinct() %>%
  rename(external_group = run_timing_group, internal_group = repunit, label = collection) %>%
  select(external_group, internal_group, label)

# get the result
TAF_FST <- table_as_figure(
  X = full_fst, 
  RC_groups = RC_groups,
  external_colors = run_time_colors,
  internal_colors = repunit_colors,
  Xs_on_diagonal = FALSE,
  plot_margins = c(0.7, 0.1, 0.1, 0.1)
)


# then plot it and add the scatter points on the lower diagonal
fst_with_scatters <- TAF_FST$full_plot +
  geom_point(
    data = pair_freqs,
    mapping = aes(x = x, y = y),
    size = 0.025,
    colour = "black"
  )


```
       
And we can save that too:
```{r}
ggsave(fst_with_scatters, file = output_list$fst_table, width = 6, height = 5.1)
```
       
# Put them together into a single figure for the paper
       
```{r}
gsi_fst_fig_for_paper <- plot_grid(
  fst_with_scatters,
  plot_grid(
    get_legend(lfar_ass_table$for_external_legend),
    get_legend(lfar_ass_table$for_internal_legend),
    nrow = 1
  ),
  lfar_ass_table_no_legend$plot,
  nrow = 3,
  rel_heights = c(6,1.5,6), 
  labels = c("a)", "", "b)"),
  label_size = 32, 
  label_x = -0.02,
  label_y = 1.013
)

# and then save that
ggsave(gsi_fst_fig_for_paper, file = output_list$gsi_fst_fig, width = 10, height = 19)

# let's crop that down for use, too
CALL <- paste("pdfcrop ", output_list$gsi_fst_fig, collapse = " ")
system(CALL)

file.rename(from = output_list$gsi_fst_fig, to = output_list$gsi_fst_fig_tex)
```

       
# Population Genetic Summaries
       
We can do all of our standard population genetic summaries here using `full_base2`.
       
It is pretty standard, but I think that i am going to standardize sample sizes.
       
```{r}
# first pivot and get whether each locus is Hz or not
fbl_10 <- full_base2 %>%
  select(-starts_with("NC_037130.1")) %>%  # drop the LFARs
  pivot_longer(
    cols = c(-(indiv:sample_type)),
    names_to = c("locus", "gene_copy"),
    names_pattern = "^(.*)_([12])$"
  ) %>%
  group_by(indiv, locus) %>%
  mutate(isHz = value[1] != value[2]) %>%
  ungroup()


# then get the proportion of missing data for each population
miss_props <- fbl_10 %>% 
  group_by(repunit, collection) %>%
  summarise(fractMiss = mean(is.na(value))) %>%
  ungroup()

  
fbl_no_miss <- fbl_10 %>%
  filter(!is.na(value) & !is.na(isHz))

# now we just summarize by collection and locus
simple_summaries <-  fbl_no_miss %>%
  group_by(repunit, collection, locus) %>%
  summarise(
    num_diploids = n() / 2.0,
    obs_Hz = mean(isHz),
    num_alleles = n_distinct(value)
  ) %>%
  ungroup()

# here, we get the minimum number of diploids at each locus, which we
# use for rarefaction sampling.
min_gene_copies <- simple_summaries %>%
  group_by(locus) %>%
  summarise(min_gc = min(num_diploids) * 2)

 


# here is a function to take N samples of size s from a vector
# and return the mean number of distinct elements in it:
subsample_num_alleles <- function(v, s, N=1000) {
  samples <- lapply(
    1:N, 
    function(z) {
      sample(x = v, size = s, replace = FALSE) %>% n_distinct()
    }) %>% unlist()
  
  
  # now, return a tibble with columns mean_num_alle_subsamp an fract_polymorphic_subsamp
  tibble(
    mean_num_alle_subsamp = mean(samples),
    fract_polymorphic_subsamp = mean(samples > 1)
  )
}

# we will now use that min_diploid number to downsample
# each collection randomly, without replacement 1000 times and 
# calculate the mean number of alleles.
set.seed(1)
sub_sampled_locus_num_alle <- fbl_no_miss %>%
  left_join(min_gene_copies, by = join_by(locus)) %>%
  group_by(repunit, collection, locus) %>%
  summarise(
    mean_num_alleles = list(subsample_num_alleles(value, min_gc[1], N = 100))
  ) %>%
  ungroup() %>%
  unnest(mean_num_alleles)

# and now we can calculate the means over loci of those
subby <- sub_sampled_locus_num_alle %>%
  group_by(repunit, collection) %>%
  summarise(
    mean_num_alle = mean(mean_num_alle_subsamp),
    mean_fract_poly = mean(fract_polymorphic_subsamp)
  )

# here we will get the estimated allele freqs and, from those, calculate
# expected heterozygosity as 1 - \sum_alleles freq^2
expHz <- fbl_no_miss %>%
  group_by(repunit, collection, locus, value) %>%
  summarise(
    n = n()
  ) %>%
  mutate(freq = n / sum(n)) %>%
  group_by(repunit, collection, locus) %>%
  summarise(exp_Hz = 1 - sum(freq ^2)) %>%
  ungroup()


# now, join that on there:
locus_summaries <- simple_summaries %>%
  left_join(expHz, by = join_by(repunit, collection, locus))

# and now we can summarise these into means and SDs
mean_Hzs_etc <- locus_summaries %>%
  group_by(repunit, collection) %>%
  summarise(
    mean_num_dip = mean(num_diploids),
    mean_expHz = mean(exp_Hz),
    mean_obsHz = mean(obs_Hz),
  )

# OK, join the subsampled numbers onto those
tibble_for_paper <- subby %>%
  left_join(mean_Hzs_etc, by = join_by(repunit, collection)) %>%
  left_join(miss_props, by = join_by(repunit, collection)) %>%
  mutate(collection = factor(collection, levels = unique(pop_labels$collection))) %>%
  arrange(collection) %>%
  ungroup() %>%
  select(collection, fractMiss, mean_num_alle, mean_fract_poly, mean_expHz, mean_obsHz) %>%
  mutate(
    collection = as.character(collection),
    fractMiss = sprintf("%.3f", fractMiss),
    mean_num_alle = sprintf("%.2f", mean_num_alle),
    mean_fract_poly = sprintf("%.2f", mean_fract_poly),
    mean_expHz = sprintf("%.3f", mean_expHz),
    mean_obsHz = sprintf("%.3f", mean_obsHz)
  ) %>%
  rename(
    Code = collection,
    `$M$` = fractMiss,
    `$\\bar{N}_\\mathrm{A,ss}$` = mean_num_alle,
    `$\\bar{P}_\\mathrm{poly,ss}$` = mean_fract_poly,
    `$\\bar{\\mathrm{H}}_\\mathrm{exp}$` = mean_expHz,
    `$\\bar{\\mathrm{H}}_\\mathrm{obs}$` = mean_obsHz,
  )

tibble_for_paper$Code[1] <- sprintf("\\hline %s", tibble_for_paper$Code[1])

# then write that table out:
write_delim(
  tibble_for_paper,
  delim = "&",
  file = output_list$popgen_summ, 
  eol = "\\tabularnewline\n", 
  quote = "none"
)
```


## Locus specific pop-gen summaries

Now, we also are going to make summaries for each locus in each collection.
Carlos wanted a supplemental data table that has Hexp and Hobs for each
collection.  I figure we might as well add the number of diploids and the
number of alleles in there as well.   All of that information is in
`locus_summaries` in a tidy format, but we will make it wide for this.
Carlos wanted to add it to the Supp-Data-1-Amplicon-info.xlsx, but I think
that is actually a little crazy because we are going to have so many columns in this
output. So, I will put it into a CSV file.
```{r}
by_locus_table <- locus_summaries %>%
  select(locus, collection, num_diploids, num_alleles, exp_Hz, obs_Hz) %>%
  pivot_wider(
    names_from = collection,
    values_from = num_diploids:obs_Hz,
    names_sep = "."
  )

write_csv(by_locus_table, file = output_list$pop_gen_by_loc_coll)
```


       
# Finally, get the histogram of the total number of alleles per locus
       
       
```{r}
numa <- fbl_no_miss %>%
  group_by(locus) %>%
  summarise(
    num_alle = n_distinct(value)
  ) %>%
  count(num_alle) %>%
  bind_rows(tibble(num_alle = 9, n = 0))


gbp <- ggplot(numa, aes(x = factor(num_alle), y = n)) +
  geom_col(fill = "blue") +
  theme_bw() +
  xlab("Total number of alleles") +
  ylab("Number of loci")

ggsave(gbp, filename = output_list$num_alle_barplot, width = 4, height = 3)

numa
```


# Session Info

```{r}
sessioninfo::session_info()
```


# Running Time

Running the code and rendering this notebook required approximately this much time:

```{r}
Sys.time() - start_time
```




